import streamlit as st 

# í˜ì´ì§€ ê¸°ë³¸ ì„¤ì • 
st.set_page_config(
    page_icon = "ğŸ—ºï¸ğŸ“",
    page_title = "BIZBUZZ",
    layout = "wide",
)

st.subheader("USA")

st.markdown("""
    <style>
    .small-font {
        font-size:13px;  # ì›í•˜ëŠ” ê¸€ì í¬ê¸°ë¡œ ì¡°ì ˆ
    }
    </style>
    <p class="small-font"> : CA - NJ - NY - TX - VA - MD - GA - WA - NC 9ê°œì˜ ì£¼ ì„ ì • </p>
    """, unsafe_allow_html=True)

# USA detailed map ë„£ê¸° 
import streamlit as st
import pandas as pd
import plotly.express as px

# ë°ìŠ¤í¬íƒ‘ì— ìˆëŠ” ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
excel_file_path = '/Users/dydit/Desktop/us_address_all.xlsx'  # ì‹¤ì œ ì‚¬ìš©ì ì´ë¦„ìœ¼ë¡œ ë³€ê²½

# ì—‘ì…€ íŒŒì¼ ì½ì–´ì˜¤ê¸° (ì»¬ëŸ¼ ì´ë¦„ ì—†ìŒ)
df = pd.read_excel(excel_file_path, header=None)

# Scatter_geoë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì¥ì†Œì— í•€ ì°ê¸°
fig = px.scatter_geo(df,
                     lat=df.iloc[:, 1].tolist(),  # ìœ„ë„ ì •ë³´ëŠ” ë‘ ë²ˆì§¸ ì—´ì— ìœ„ì¹˜
                     lon=df.iloc[:, 2].tolist(),  # ê²½ë„ ì •ë³´ëŠ” ì„¸ ë²ˆì§¸ ì—´ì— ìœ„ì¹˜
                     scope='usa',
                     title='DETAILED USA MAP',
                     )

st.plotly_chart(fig, use_container_width=True)

# Colored USA Map ë„£ê¸°
# ì—‘ì…€ íŒŒì¼ì˜ ë¡œì»¬ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”.
excel_file_path = '/Users/dydit/Desktop/us_address_all.xlsx'

# ì—‘ì…€ íŒŒì¼ ì½ì–´ì˜¤ê¸° (ì»¬ëŸ¼ ì´ë¦„ ì—†ìŒ)
df = pd.read_excel(excel_file_path, header=None)

# Scatter_geoë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ì¥ì†Œì— í•€ ì°ê¸°
fig = px.scatter_geo(df,
                     lat=df.iloc[:, 1].tolist(),
                     lon=df.iloc[:, 2].tolist(),
                     scope='usa',
                     title='COLORED USA MAP')

# ì£¼ë³„ í•€ì˜ ê°œìˆ˜ ê³„ì‚°
pin_counts = df.iloc[:, 3].value_counts().reset_index()
pin_counts.columns = ['State', 'Pin Count']

# Choropleth mapì„ ì‚¬ìš©í•˜ì—¬ ë¯¸êµ­ ì£¼ì— ëŒ€í•œ í•€ì˜ ê°œìˆ˜ í‘œì‹œ (ë¶‰ì€ ê³„ì—´ ìƒ‰ìƒ ë§µ ì‚¬ìš©)
choropleth_fig = px.choropleth(pin_counts,
                               locations='State',
                               locationmode='USA-states',
                               color='Pin Count',
                               scope='usa',
                               title='Choropleth USA Map - Pin Counts',
                               color_continuous_scale='YlOrRd')

# ë‘ ê·¸ë˜í”„ë¥¼ ë³‘í•©í•˜ê¸°
for trace in choropleth_fig.data:
    fig.add_trace(trace)

# Streamlitì—ì„œ ê·¸ë˜í”„ í‘œì‹œ
st.plotly_chart(fig, use_container_width=True)

# ë°” ì°¨íŠ¸ ìƒì„±
bar_chart_fig = px.bar(pin_counts,
                       x='State',
                       y='Pin Count',
                       title='Pin Counts by States',
                       labels={'Pin Count': 'Count', 'State': 'State'},
                       color_discrete_sequence=['#FF0000'])  # ë¹¨ê°„ìƒ‰ ê³„ì—´ì˜ ìƒ‰ìƒ

# Streamlitì—ì„œ ë°” ì°¨íŠ¸ í‘œì‹œ
st.plotly_chart(bar_chart_fig, use_container_width=True)





# ì½”ë“œ ë³´ì—¬ì¤„ ë•Œ (ì˜ˆì˜ê²Œ)
if st.button("USA python code ë³´ê¸°"):
    code = '''
# ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ ì •ì˜
def run_code():
    articles = []
    error_list = []

    url_1 = 'https://www.state.gov/press-releases/'
    wd = initialize_chrome_driver()
    wd.get(url_1)
    time.sleep(3)
    html = wd.page_source
    soup = BeautifulSoup(html, 'html.parser')
    error_message = str()
    
    try:
        news_items = soup.find_all('li', class_='collection-result')
        for item in news_items:
            error_message = ''
            link = item.find('a', class_='collection-result__link')['href']
            if not link:
             error_message = Error_Message(error_message, "None Link")
            date_tag = item.find('div', class_='collection-result-meta').find('span', dir='ltr')
            extracted_date = date_tag.text.strip() if date_tag else 'No date found'
            article_date = date_util(extracted_date)
            if not date_tag:
                error_message = Error_Message(error_message, "None date")
            if article_date <= today:
                # newspaper : ì œëª©,ë³¸ë¬¸ ì¶”ì¶œ
                article = Article(link, language='en')
                article.download()
                article.parse()
                title = article.title
                if not title:
                    error_message = Error_Message(error_message, "None Link")
                text = article.text
                if not text:
                    error_message = Error_Message(error_message, "None Content")
                if error_message is not str():
                    error_list.append({
                            'Error Link': url_1,
                            'Error': error_message
                        })
                else:
                    articles.append({
                            'Title': title,
                            'Link': link,
                            'Content(RAW)': text,
                        })
    except Exception as e:
        error_list.append({
            'Error Link': url_1,
            'Error': str(e)
            })
    return articles, error_list
        

# ì½”ë“œ ì‹¤í–‰ 
if st.button("USA python ì½”ë“œ ì‹¤í–‰"):
    # run_code í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  ë°˜í™˜ëœ articlesë¥¼ ë°›ìŠµë‹ˆë‹¤.
    articles_data, error_data = run_code()
    
    # ë°˜í™˜ëœ articles ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¼ë¦¿ì˜ dataframeìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    st.dataframe(articles_data)
    '''
    st.code(code, language = "python")


# ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
import time
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
import re
from newspaper import Article
import requests
import numpy as np
from selenium.webdriver.common.by import By

def initialize_chrome_driver():
  # Chrome ì˜µì…˜ ì„¤ì • : USER_AGENTëŠ” ì•Œì•„ì„œ ìˆ˜ì •
  #USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.6045.105 Safari/537.36"
  # íƒœì¤€ì»´
  USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5.2 Safari/605.1.15'
  chrome_options = Options()
  chrome_options.page_load_strategy = 'normal'  # 'none', 'eager', 'normal'
  chrome_options.add_argument('--headless')
  chrome_options.add_argument('--no-sandbox')
  chrome_options.add_argument('--disable-dev-shm-usage')
  chrome_options.add_argument('--disable-gpu')
  chrome_options.add_argument(f'user-agent={USER_AGENT}')
  # Chrome ë“œë¼ì´ë²„ ì„¤ì •
  service = Service()
  wd = webdriver.Chrome(service=service, options=chrome_options)
  return wd

# ë‚ ì§œ í†µí•© í•¨ìˆ˜
def date_util(article_date):
  try:
    # Parse the date using dateutil.parser
    article_date = parser.parse(article_date).date()
  except ValueError:
    # If parsing fails, handle the relative dates
    article_date = article_date.lower()
    time_keywords = ["h", "hrs", "hr", "m", "s", "hours","hour", "minutes", "minute", "mins", "min", "seconds", "second", "secs", "sec"]
    if any(keyword in article_date for keyword in time_keywords):
      article_date = today
    elif "days" in article_date or "day" in article_date:
      # Find the number of days and subtract from today
      number_of_days = int(''.join(filter(str.isdigit, article_date)))
      article_date = today - timedelta(days=number_of_days)
    else:
      return None
  return article_date

today = date_util(datetime.now().strftime("%Y-%m-%d"))

# ì—ëŸ¬ ë©”ì‹œì§€ ì‘ì„± í•¨ìˆ˜
def Error_Message(message, add_error):
    if message is not str() : message += '/'
    message += add_error
    return message

# ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜ ì •ì˜
def run_code():
    articles = []
    error_list = []

    url_1 = 'https://www.state.gov/press-releases/'
    wd = initialize_chrome_driver()
    wd.get(url_1)
    time.sleep(3)
    html = wd.page_source
    soup = BeautifulSoup(html, 'html.parser')
    error_message = str()
    
    try:
        news_items = soup.find_all('li', class_='collection-result')
        for item in news_items:
            error_message = ''
            link = item.find('a', class_='collection-result__link')['href']
            if not link:
             error_message = Error_Message(error_message, "None Link")
            date_tag = item.find('div', class_='collection-result-meta').find('span', dir='ltr')
            extracted_date = date_tag.text.strip() if date_tag else 'No date found'
            article_date = date_util(extracted_date)
            if not date_tag:
                error_message = Error_Message(error_message, "None date")
            if article_date <= today:
                # newspaper : ì œëª©,ë³¸ë¬¸ ì¶”ì¶œ
                article = Article(link, language='en')
                article.download()
                article.parse()
                title = article.title
                if not title:
                    error_message = Error_Message(error_message, "None Link")
                text = article.text
                if not text:
                    error_message = Error_Message(error_message, "None Content")
                if error_message is not str():
                    error_list.append({
                            'Error Link': url_1,
                            'Error': error_message
                        })
                else:
                    articles.append({
                            'Title': title,
                            'Link': link,
                            'Content(RAW)': text,
                        })
    except Exception as e:
        error_list.append({
            'Error Link': url_1,
            'Error': str(e)
            })
    return articles, error_list
        



# ì½”ë“œ ì‹¤í–‰ 
if st.button("USA python code ì‹¤í–‰"):
    # run_code í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê³  ë°˜í™˜ëœ articlesë¥¼ ë°›ìŠµë‹ˆë‹¤.
    articles_data, error_data = run_code()
    
    # ë°˜í™˜ëœ articles ë°ì´í„°ë¥¼ ìŠ¤íŠ¸ë¦¼ë¦¿ì˜ dataframeìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    st.dataframe(articles_data)